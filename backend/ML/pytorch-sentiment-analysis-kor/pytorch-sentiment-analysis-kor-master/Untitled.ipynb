{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n pytorch python=3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from utils import convert_to_dataset\n",
    "from torchtext import data as ttd\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from soynlp.word import WordExtractor\n",
    "\n",
    "\n",
    "def build_tokenizer():\n",
    "    \"\"\"\n",
    "    Train soynlp tokenizer which will be used to tokenize Korean input sentence\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    print(f'Now building soynlp tokenizer . . .')\n",
    "\n",
    "    data_dir = Path().cwd() / 'data'\n",
    "    train_txt = os.path.join(data_dir, 'train.txt')\n",
    "\n",
    "    with open(train_txt, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    word_extractor = WordExtractor(min_frequency=5)\n",
    "    word_extractor.train(lines)\n",
    "\n",
    "    word_scores = word_extractor.extract()\n",
    "    cohesion_scores = {word: score.cohesion_forward for word, score in word_scores.items()}\n",
    "\n",
    "    with open('pickles/tokenizer.pickle', 'wb') as pickle_out:\n",
    "        pickle.dump(cohesion_scores, pickle_out)\n",
    "\n",
    "\n",
    "def build_vocab(config):\n",
    "    \"\"\"\n",
    "    Build vocab used to convert Korean input sentence into word indices using soynlp tokenizer\n",
    "    Args:\n",
    "        config: configuration object containing various options\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pickle_tokenizer = open('pickles/tokenizer.pickle', 'rb')\n",
    "    cohesion_scores = pickle.load(pickle_tokenizer)\n",
    "    tokenizer = LTokenizer(scores=cohesion_scores)\n",
    "\n",
    "    # To use packed padded sequences, tell the model how long the actual sequences are by 'include_lengths=True'\n",
    "    text = ttd.Field(tokenize=tokenizer.tokenize, include_lengths=True)\n",
    "    label = ttd.LabelField(dtype=torch.float)\n",
    "\n",
    "    data_dir = Path().cwd() / 'data'\n",
    "    train_txt = os.path.join(data_dir, 'train.txt')\n",
    "\n",
    "    train_data = pd.read_csv(train_txt, sep='\\t')\n",
    "    train_data, valid_data = train_test_split(train_data, test_size=0.3, random_state=32)\n",
    "    train_data = convert_to_dataset(train_data, text, label)\n",
    "\n",
    "    print(f'Building vocabulary using torchtext . . .')\n",
    "    text.build_vocab(train_data, max_size=config.vocab_size)\n",
    "    label.build_vocab(train_data)\n",
    "\n",
    "    print(f'Unique tokens in TEXT vocabulary: {len(text.vocab)}')\n",
    "    print(f'Unique tokens in LABEL vocabulary: {len(label.vocab)}')\n",
    "\n",
    "    print(f'Most commonly used words are as follows:')\n",
    "    print(text.vocab.freqs.most_common(20))\n",
    "\n",
    "    file_text = open('pickles/text.pickle', 'wb')\n",
    "    pickle.dump(text, file_text)\n",
    "\n",
    "    file_label = open('pickles/label.pickle', 'wb')\n",
    "    pickle.dump(label, file_label)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Pickle Builder')\n",
    "\n",
    "    parser.add_argument('--vocab_size', type=int, default=25000)\n",
    "\n",
    "    config = parser.parse_args()\n",
    "\n",
    "    build_tokenizer()\n",
    "    build_vocab(config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
