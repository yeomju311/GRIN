{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T09:33:27.770561Z",
     "start_time": "2020-07-31T09:33:27.754630Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import codecs\n",
    "import time\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import FastText\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"util\")\n",
    "from util.load_data import load_word2vec\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 감정 댓글 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T09:01:40.520581Z",
     "start_time": "2020-07-31T09:01:37.211Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_path = \"dataset/embedding/0710 w2v_model\"\n",
    "data_path = 'dataset/final_data/0710 comment_with_emo_over0.5.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T09:01:40.522575Z",
     "start_time": "2020-07-31T09:01:37.444Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion_comment_df:  (408607, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>happy</th>\n",
       "      <th>sad</th>\n",
       "      <th>disgust</th>\n",
       "      <th>angry</th>\n",
       "      <th>surprised</th>\n",
       "      <th>fear</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment_token</th>\n",
       "      <th>check</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1238681</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>879865752DFF46EA875B6A633D3A301A</td>\n",
       "      <td>싫음말지 출세동앗줄이라고 잡아놓고 어디 정권탓하고있냐  저런검사놈들 땜에 죽은 사람...</td>\n",
       "      <td>[싫다, 말, 출세, 앗줄, 잡다, 어디, 정권, 탓, 있다, 검사, 놈, 땜, 죽...</td>\n",
       "      <td>[싫다, 싫다, 놈]</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038828</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D70F32C0ADF9444098426E63D0BDF8D3</td>\n",
       "      <td>재활용도 못하겠다! 썩어빠진년놈들 사진 실려서</td>\n",
       "      <td>[재활용, 썩다, 빠지다, 년놈, 사진, 실리다]</td>\n",
       "      <td>[썩다]</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439621</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85C7864BF18647B2AD7BCCFB320E493C</td>\n",
       "      <td>별 미친</td>\n",
       "      <td>[별, 미치다]</td>\n",
       "      <td>[미치다]</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985622</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9FE0CE98B01B4DC58720EB1F0A4A78B0</td>\n",
       "      <td>죄도 없는 대통령을 감옥에 집어넣고 죄를 뒤집어 씌우기 위해  없는 죄를 만들어 낼...</td>\n",
       "      <td>[죄, 없다, 대통령, 감옥, 집어넣다, 죄, 뒤지다, 씌우다, 위해, 없다, 죄,...</td>\n",
       "      <td>[놈, 나쁘다, 놈, 나쁘다, 놈]</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285755</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2950ECE6FE5046AF99FB716DC6BAC0BE</td>\n",
       "      <td>.이것이 우리의 정확한 위치다....    언젠가부터 우리는 우리 스스로를 선진국이...</td>\n",
       "      <td>[우리, 정확하다, 위치, 언젠가, 부터, 우리, 우리, 스스로, 선진국, 부르다,...</td>\n",
       "      <td>[경악]</td>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         happy  sad   disgust     angry  surprised  fear  \\\n",
       "1238681    0.0  0.0  0.333333  0.666667        0.0   0.0   \n",
       "1038828    0.0  0.0  0.000000  1.000000        0.0   0.0   \n",
       "1439621    0.0  0.0  0.000000  1.000000        0.0   0.0   \n",
       "985622     0.0  0.0  0.000000  1.000000        0.0   0.0   \n",
       "285755     0.0  0.0  0.000000  0.000000        1.0   0.0   \n",
       "\n",
       "                               comment_id  \\\n",
       "1238681  879865752DFF46EA875B6A633D3A301A   \n",
       "1038828  D70F32C0ADF9444098426E63D0BDF8D3   \n",
       "1439621  85C7864BF18647B2AD7BCCFB320E493C   \n",
       "985622   9FE0CE98B01B4DC58720EB1F0A4A78B0   \n",
       "285755   2950ECE6FE5046AF99FB716DC6BAC0BE   \n",
       "\n",
       "                                                   comment  \\\n",
       "1238681  싫음말지 출세동앗줄이라고 잡아놓고 어디 정권탓하고있냐  저런검사놈들 땜에 죽은 사람...   \n",
       "1038828                          재활용도 못하겠다! 썩어빠진년놈들 사진 실려서   \n",
       "1439621                                               별 미친   \n",
       "985622   죄도 없는 대통령을 감옥에 집어넣고 죄를 뒤집어 씌우기 위해  없는 죄를 만들어 낼...   \n",
       "285755   .이것이 우리의 정확한 위치다....    언젠가부터 우리는 우리 스스로를 선진국이...   \n",
       "\n",
       "                                             comment_token  \\\n",
       "1238681  [싫다, 말, 출세, 앗줄, 잡다, 어디, 정권, 탓, 있다, 검사, 놈, 땜, 죽...   \n",
       "1038828                        [재활용, 썩다, 빠지다, 년놈, 사진, 실리다]   \n",
       "1439621                                           [별, 미치다]   \n",
       "985622   [죄, 없다, 대통령, 감옥, 집어넣다, 죄, 뒤지다, 씌우다, 위해, 없다, 죄,...   \n",
       "285755   [우리, 정확하다, 위치, 언젠가, 부터, 우리, 우리, 스스로, 선진국, 부르다,...   \n",
       "\n",
       "                       check    emotion  \n",
       "1238681          [싫다, 싫다, 놈]      angry  \n",
       "1038828                 [썩다]      angry  \n",
       "1439621                [미치다]      angry  \n",
       "985622   [놈, 나쁘다, 놈, 나쁘다, 놈]      angry  \n",
       "285755                  [경악]  surprised  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emotion_comment_df = pd.read_pickle(data_path)\n",
    "    \n",
    "print(\"emotion_comment_df: \", emotion_comment_df.shape)\n",
    "display(emotion_comment_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T09:01:40.523573Z",
     "start_time": "2020-07-31T09:01:37.963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플링 전 : Counter({'angry': 240971, 'sad': 83041, 'happy': 44682, 'disgust': 17263, 'fear': 14649, 'surprised': 8001})\n",
      "\n",
      "샘플링 이후 : Counter({'angry': 8000, 'disgust': 8000, 'fear': 8000, 'happy': 8000, 'sad': 8000, 'surprised': 8000})\n"
     ]
    }
   ],
   "source": [
    "print(\"샘플링 전 :\",Counter(emotion_comment_df.emotion))\n",
    "print()\n",
    "\n",
    "# 각 감정별로 셔플한 다음 8,000개씩 부여 (Train = 85% / Test = 15%)\n",
    "sample_count = 8000\n",
    "\n",
    "sampled_emo_cmt_df = (\n",
    "    emotion_comment_df.groupby('emotion').apply(\n",
    "        lambda d: shuffle(d, random_state=42, n_samples=sample_count)).reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"샘플링 이후 :\",Counter(sampled_emo_cmt_df.emotion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**균등한 데이터셋 분포를 위해 사전에 감정 별 데이터 크기를 동일하게 맞춤**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Encoding\n",
    "\n",
    "- 모든 단어를 정수로 인코딩\n",
    "- 케라스의 `Tokenizer` 클래스 사용\n",
    "- 모든 단어를 각기 다른 단어 ID에 매핑. ID는 1부터 시작해 고유한 단어 개수까지 생성 (**마스킹에 사용하기 때문에 0부터 시작하지 않음**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T09:01:40.525567Z",
     "start_time": "2020-07-31T09:01:39.170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample data :  ['멀다', '놈', '기사', '도통', '내용', '뭐', '알다', '없다']\n",
      "texts to sequences ===> [[449, 18, 52, 8897, 729, 35, 28, 3]]\n",
      "sequences to texts ===> ['있다 되다 없다 보다 아니다']\n",
      "\n",
      "고유 단어 개수 : 35670, 전체 댓글 개수 : 48000\n"
     ]
    }
   ],
   "source": [
    "emotion_token_list = sampled_emo_cmt_df[\"comment_token\"].tolist()\n",
    "\n",
    "\n",
    "# 모든 글자를 정수 ID로 인코딩\n",
    "def word_to_id(token_list):\n",
    "    print(\"sample data : \",token_list[0])\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(token_list)\n",
    "    print(\"texts to sequences ===>\",tokenizer.texts_to_sequences([token_list[0]]))\n",
    "    print(\"sequences to texts ===>\",tokenizer.sequences_to_texts([[1,2,3,4,5]])) \n",
    "    print()\n",
    "    \n",
    "    word_to_id = tokenizer.word_index\n",
    "    id_to_word = tokenizer.index_word\n",
    "    \n",
    "    print(\"고유 단어 개수 : {}, 전체 댓글 개수 : {}\".format(len(word_to_id), tokenizer.document_count))\n",
    "    \n",
    "    encoded_tokens = []\n",
    "    for token in token_list:\n",
    "        encoded_tokens.append(tokenizer.texts_to_sequences([token])[0])\n",
    "    \n",
    "    return encoded_tokens, word_to_id, id_to_word\n",
    "    \n",
    "encoded_tokens, word_to_id, id_to_word = word_to_id(emotion_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T09:01:40.527561Z",
     "start_time": "2020-07-31T09:01:39.987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (48000, 20)\n",
      "Shape of label tensor: (48000, 6)\n",
      "\n",
      "[array(['angry', 'disgust', 'fear', 'happy', 'sad', 'surprised'],\n",
      "      dtype=object)]\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# X (padding)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "\n",
    "data = tf.keras.preprocessing.sequence.pad_sequences(encoded_tokens, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# y (one-hot encoding labels)\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "labels = cat_encoder.fit_transform(sampled_emo_cmt_df[['emotion']]).toarray()\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print()\n",
    "print(cat_encoder.categories_)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Embedding Matrix 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T09:01:42.858356Z",
     "start_time": "2020-07-31T09:01:42.848383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec 모델 불러오기 ... finished in 0.58 sec.\n",
      "#words = 168620, vector size = 300\n"
     ]
    }
   ],
   "source": [
    "w2v_model = load_word2vec(word2vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T09:01:43.056801Z",
     "start_time": "2020-07-31T09:01:43.051814Z"
    }
   },
   "outputs": [],
   "source": [
    "# with pretrained model\n",
    "def create_embedding_matrix(w2v_model, word_to_id):\n",
    "    '''\n",
    "    pretrained model이 존재할 때 embedding matrix 만드는법\n",
    "    if 존재하지 않는다면, tf.keras.Embedding layer 사용\n",
    "    '''\n",
    "    \n",
    "    EMBEDDING_DIM = w2v_model.vector_size\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_to_id)+1, EMBEDDING_DIM))\n",
    "    print(embedding_matrix.shape)\n",
    "\n",
    "    for word, i in word_to_id.items():\n",
    "        try:\n",
    "            embedding_vector = w2v_model.wv[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            embedding_matrix[i] = np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T09:01:43.314113Z",
     "start_time": "2020-07-31T09:01:43.305137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35671, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.13877146, -0.03067526, -0.07808419, ..., -0.05337833,\n",
       "        -0.0992438 , -0.00218738],\n",
       "       [-0.03504737, -0.12057228,  0.01615535, ...,  0.005222  ,\n",
       "        -0.08698165,  0.06102141],\n",
       "       ...,\n",
       "       [ 0.04774499,  0.08327273,  0.00324909, ..., -0.05244453,\n",
       "        -0.05709571, -0.09028624],\n",
       "       [-0.05991533, -0.03845508, -0.01238701, ..., -0.00123169,\n",
       "        -0.07324471, -0.04061742],\n",
       "       [-0.11260591, -0.00158114,  0.01082337, ...,  0.00661842,\n",
       "        -0.05145192, -0.09587358]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = create_embedding_matrix(w2v_model, word_to_id)\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(38400, 20) \n",
      "Validation set: \t(4800, 20) \n",
      "Test set: \t\t(4800, 20)\n"
     ]
    }
   ],
   "source": [
    "# shuffle data \n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# split the data into a training set and a validation set \n",
    "split_frac=0.8\n",
    "split_idx = int(len(data)*split_frac)\n",
    "X_train, remaining_x = data[:split_idx], data[split_idx:]\n",
    "y_train, remaining_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "X_val, X_test = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "y_val, y_test = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(X_train.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(X_val.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. CNN 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LEGNTH = 20\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "dim_output = 6\n",
    "\n",
    "class textCNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embedding_matrix,\n",
    "                 output_size=dim_output,\n",
    "                 max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "                 num_filters=100,\n",
    "                 filter_sizes=[3,4,5],\n",
    "                 drop_prob=0.5,\n",
    "                 train_embedding=False, # Note that we set trainable=False to prevent the weights from being updated during training.\n",
    "                 **kwargs):\n",
    "        super(textCNN,self).__init__(**kwargs) # 표준 매개변수 처리\n",
    "        \n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.output_size = output_size\n",
    "\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        \n",
    "        #### layers\n",
    "        # if using pretrained model to initialize\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
    "                                                    EMBEDDING_DIM,\n",
    "                                                    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                                                    trainable=train_embedding) \n",
    "\n",
    "        # # if not using pretrained model\n",
    "        # self.embedding_layer = tf.keras.layers.Embedding(len(word_to_id) + 1,\n",
    "        #                                               EMBEDDING_DIM,\n",
    "        #                                               input_length=MAX_SEQUENCE_LENGTH) \n",
    "\n",
    "        self.conv_layers = []\n",
    "        for i,filter_size in enumerate(filter_sizes):\n",
    "            conv_block = tf.keras.Sequential()\n",
    "\n",
    "            conv = tf.keras.layers.Conv2D(num_filters,\n",
    "                                       (filter_size, embedding_dim),\n",
    "                                       padding='VALID', # ignore padding\n",
    "                                       activation='relu',\n",
    "                                       strides=(1,1),\n",
    "                                       name=f'conv_layer_{i}')\n",
    "\n",
    "            pooling = tf.keras.layers.MaxPooling2D((max_sequence_length - filter_size + 1, 1),\n",
    "                                                padding='valid',\n",
    "                                                strides=(1,1),\n",
    "                                                name=f'pooling_layer_{i}')    \n",
    "\n",
    "            conv_block.add(conv)\n",
    "            conv_block.add(pooling)\n",
    "            \n",
    "            self.conv_layers.append(conv_block)\n",
    "            \n",
    "        self.dense = tf.keras.layers.Dense(output_size,\n",
    "                                     activation='softmax',\n",
    "                                     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                     name='prediction')\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten(name='flatten')\n",
    "        self.reshape = tf.keras.layers.Reshape((max_sequence_length, embedding_dim, 1))\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=drop_prob, name='dropout')\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        \n",
    "        embedding = self.embedding_layer(inputs)\n",
    "        reshape = self.reshape(embedding) #(sequence_length,EMBEDDING_DIM,1)\n",
    "        \n",
    "        pool_outputs = [layer(reshape) for layer in self.conv_layers]\n",
    "        pool_outputs = tf.keras.layers.concatenate(pool_outputs, axis=-1, name='concatenate')\n",
    "        pool_outputs = self.flatten(pool_outputs)\n",
    "        pool_outputs = self.dropout(pool_outputs)\n",
    "\n",
    "        if training:\n",
    "            pool_outputs=self.dropout(pool_outputs)\n",
    "\n",
    "        logit = self.dense(pool_outputs)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = textCNN(embedding_matrix = embedding_matrix,\n",
    "              max_sequence_length=20,\n",
    "              output_size = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss='categorical_crossentropy',\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            metrics=['acc'])\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.4104 - acc: 0.8992 - val_loss: 0.3599 - val_acc: 0.9062\n",
      "Epoch 2/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.3828 - acc: 0.9066 - val_loss: 0.3440 - val_acc: 0.9110\n",
      "Epoch 3/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.3619 - acc: 0.9119 - val_loss: 0.3319 - val_acc: 0.9123\n",
      "Epoch 4/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.3437 - acc: 0.9167 - val_loss: 0.3249 - val_acc: 0.9121\n",
      "Epoch 5/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.3296 - acc: 0.9198 - val_loss: 0.3201 - val_acc: 0.9135\n",
      "Epoch 6/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.3192 - acc: 0.9241 - val_loss: 0.3147 - val_acc: 0.9135\n",
      "Epoch 7/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.3094 - acc: 0.9273 - val_loss: 0.3073 - val_acc: 0.9162\n",
      "Epoch 8/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.2954 - acc: 0.9317 - val_loss: 0.3053 - val_acc: 0.9154\n",
      "Epoch 9/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.2885 - acc: 0.9340 - val_loss: 0.3012 - val_acc: 0.9152\n",
      "Epoch 10/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.2781 - acc: 0.9362 - val_loss: 0.2983 - val_acc: 0.9192\n",
      "Epoch 11/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.2716 - acc: 0.9389 - val_loss: 0.2968 - val_acc: 0.9210\n",
      "Epoch 12/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.2666 - acc: 0.9405 - val_loss: 0.2953 - val_acc: 0.9175\n",
      "Epoch 13/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.2593 - acc: 0.9429 - val_loss: 0.2916 - val_acc: 0.9190\n",
      "Epoch 14/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.2523 - acc: 0.9458 - val_loss: 0.2921 - val_acc: 0.9200\n",
      "Epoch 15/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.2486 - acc: 0.9474 - val_loss: 0.2896 - val_acc: 0.9202\n",
      "Epoch 16/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.2428 - acc: 0.9492 - val_loss: 0.2912 - val_acc: 0.9206\n",
      "Epoch 17/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.2384 - acc: 0.9509 - val_loss: 0.2883 - val_acc: 0.9221\n",
      "Epoch 18/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.2349 - acc: 0.9521 - val_loss: 0.2870 - val_acc: 0.9210\n",
      "Epoch 19/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.2287 - acc: 0.9541 - val_loss: 0.2868 - val_acc: 0.9208\n",
      "Epoch 20/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.2255 - acc: 0.9552 - val_loss: 0.2852 - val_acc: 0.9200\n",
      "Epoch 21/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.2196 - acc: 0.9580 - val_loss: 0.2851 - val_acc: 0.9200\n",
      "Epoch 22/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.2166 - acc: 0.9580 - val_loss: 0.2841 - val_acc: 0.9225\n",
      "Epoch 23/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.2113 - acc: 0.9603 - val_loss: 0.2848 - val_acc: 0.9215\n",
      "Epoch 24/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.2109 - acc: 0.9610 - val_loss: 0.2834 - val_acc: 0.9200\n",
      "Epoch 25/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.2049 - acc: 0.9630 - val_loss: 0.2803 - val_acc: 0.9212\n",
      "Epoch 26/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1989 - acc: 0.9630 - val_loss: 0.2805 - val_acc: 0.9219\n",
      "Epoch 27/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1989 - acc: 0.9643 - val_loss: 0.2811 - val_acc: 0.9217\n",
      "Epoch 28/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1962 - acc: 0.9650 - val_loss: 0.2821 - val_acc: 0.9225\n",
      "Epoch 29/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1949 - acc: 0.9647 - val_loss: 0.2812 - val_acc: 0.9219\n",
      "Epoch 30/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1905 - acc: 0.9661 - val_loss: 0.2804 - val_acc: 0.9219\n",
      "Epoch 31/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1857 - acc: 0.9686 - val_loss: 0.2798 - val_acc: 0.9235\n",
      "Epoch 32/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1846 - acc: 0.9678 - val_loss: 0.2792 - val_acc: 0.9212\n",
      "Epoch 33/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1808 - acc: 0.9693 - val_loss: 0.2804 - val_acc: 0.9221\n",
      "Epoch 34/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1797 - acc: 0.9702 - val_loss: 0.2807 - val_acc: 0.9206\n",
      "Epoch 35/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1757 - acc: 0.9720 - val_loss: 0.2793 - val_acc: 0.9231\n",
      "Epoch 36/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1748 - acc: 0.9709 - val_loss: 0.2799 - val_acc: 0.9219\n",
      "Epoch 37/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1717 - acc: 0.9726 - val_loss: 0.2801 - val_acc: 0.9212\n",
      "Epoch 38/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1723 - acc: 0.9725 - val_loss: 0.2772 - val_acc: 0.9227\n",
      "Epoch 39/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1684 - acc: 0.9733 - val_loss: 0.2792 - val_acc: 0.9227\n",
      "Epoch 40/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1655 - acc: 0.9734 - val_loss: 0.2802 - val_acc: 0.9208\n",
      "Epoch 41/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1657 - acc: 0.9737 - val_loss: 0.2780 - val_acc: 0.9227\n",
      "Epoch 42/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1623 - acc: 0.9751 - val_loss: 0.2772 - val_acc: 0.9233\n",
      "Epoch 43/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1617 - acc: 0.9741 - val_loss: 0.2759 - val_acc: 0.9219\n",
      "Epoch 44/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1575 - acc: 0.9747 - val_loss: 0.2796 - val_acc: 0.9219\n",
      "Epoch 45/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1580 - acc: 0.9758 - val_loss: 0.2753 - val_acc: 0.9231\n",
      "Epoch 46/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1572 - acc: 0.9767 - val_loss: 0.2773 - val_acc: 0.9219\n",
      "Epoch 47/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1552 - acc: 0.9764 - val_loss: 0.2768 - val_acc: 0.9212\n",
      "Epoch 48/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1542 - acc: 0.9771 - val_loss: 0.2763 - val_acc: 0.9215\n",
      "Epoch 49/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1521 - acc: 0.9767 - val_loss: 0.2779 - val_acc: 0.9212\n",
      "Epoch 50/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1494 - acc: 0.9789 - val_loss: 0.2757 - val_acc: 0.9237\n",
      "Epoch 51/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1487 - acc: 0.9779 - val_loss: 0.2741 - val_acc: 0.9246\n",
      "Epoch 52/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1473 - acc: 0.9777 - val_loss: 0.2728 - val_acc: 0.9225\n",
      "Epoch 53/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1481 - acc: 0.9786 - val_loss: 0.2740 - val_acc: 0.9237\n",
      "Epoch 54/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1448 - acc: 0.9792 - val_loss: 0.2759 - val_acc: 0.9219\n",
      "Epoch 55/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1430 - acc: 0.9787 - val_loss: 0.2740 - val_acc: 0.9225\n",
      "Epoch 56/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1420 - acc: 0.9798 - val_loss: 0.2747 - val_acc: 0.9227\n",
      "Epoch 57/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1422 - acc: 0.9798 - val_loss: 0.2738 - val_acc: 0.9229\n",
      "Epoch 58/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1410 - acc: 0.9791 - val_loss: 0.2753 - val_acc: 0.9235\n",
      "Epoch 59/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1394 - acc: 0.9809 - val_loss: 0.2758 - val_acc: 0.9223\n",
      "Epoch 60/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1376 - acc: 0.9808 - val_loss: 0.2743 - val_acc: 0.9229\n",
      "Epoch 61/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1382 - acc: 0.9805 - val_loss: 0.2758 - val_acc: 0.9210\n",
      "Epoch 62/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1367 - acc: 0.9809 - val_loss: 0.2736 - val_acc: 0.9212\n",
      "Epoch 63/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1338 - acc: 0.9813 - val_loss: 0.2738 - val_acc: 0.9225\n",
      "Epoch 64/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1334 - acc: 0.9815 - val_loss: 0.2740 - val_acc: 0.9225\n",
      "Epoch 65/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1340 - acc: 0.9816 - val_loss: 0.2736 - val_acc: 0.9217\n",
      "Epoch 66/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1316 - acc: 0.9818 - val_loss: 0.2718 - val_acc: 0.9215\n",
      "Epoch 67/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1317 - acc: 0.9826 - val_loss: 0.2754 - val_acc: 0.9217\n",
      "Epoch 68/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1300 - acc: 0.9826 - val_loss: 0.2739 - val_acc: 0.9233\n",
      "Epoch 69/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1283 - acc: 0.9830 - val_loss: 0.2719 - val_acc: 0.9204\n",
      "Epoch 70/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1289 - acc: 0.9824 - val_loss: 0.2722 - val_acc: 0.9223\n",
      "Epoch 71/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1263 - acc: 0.9837 - val_loss: 0.2728 - val_acc: 0.9217\n",
      "Epoch 72/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1263 - acc: 0.9831 - val_loss: 0.2713 - val_acc: 0.9212\n",
      "Epoch 73/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1254 - acc: 0.9836 - val_loss: 0.2721 - val_acc: 0.9223\n",
      "Epoch 74/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1249 - acc: 0.9825 - val_loss: 0.2744 - val_acc: 0.9223\n",
      "Epoch 75/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1230 - acc: 0.9837 - val_loss: 0.2731 - val_acc: 0.9217\n",
      "Epoch 76/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1249 - acc: 0.9838 - val_loss: 0.2711 - val_acc: 0.9231\n",
      "Epoch 77/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1218 - acc: 0.9845 - val_loss: 0.2711 - val_acc: 0.9221\n",
      "Epoch 78/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1214 - acc: 0.9842 - val_loss: 0.2742 - val_acc: 0.9219\n",
      "Epoch 79/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1199 - acc: 0.9836 - val_loss: 0.2715 - val_acc: 0.9231\n",
      "Epoch 80/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1198 - acc: 0.9845 - val_loss: 0.2710 - val_acc: 0.9225\n",
      "Epoch 81/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1173 - acc: 0.9854 - val_loss: 0.2731 - val_acc: 0.9219\n",
      "Epoch 82/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1171 - acc: 0.9846 - val_loss: 0.2718 - val_acc: 0.9227\n",
      "Epoch 83/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1174 - acc: 0.9851 - val_loss: 0.2730 - val_acc: 0.9221\n",
      "Epoch 84/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1174 - acc: 0.9846 - val_loss: 0.2698 - val_acc: 0.9248\n",
      "Epoch 85/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1175 - acc: 0.9849 - val_loss: 0.2724 - val_acc: 0.9229\n",
      "Epoch 86/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1128 - acc: 0.9858 - val_loss: 0.2703 - val_acc: 0.9237\n",
      "Epoch 87/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1151 - acc: 0.9854 - val_loss: 0.2714 - val_acc: 0.9233\n",
      "Epoch 88/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1128 - acc: 0.9856 - val_loss: 0.2734 - val_acc: 0.9221\n",
      "Epoch 89/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1111 - acc: 0.9866 - val_loss: 0.2694 - val_acc: 0.9237\n",
      "Epoch 90/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1146 - acc: 0.9849 - val_loss: 0.2706 - val_acc: 0.9233\n",
      "Epoch 91/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1111 - acc: 0.9864 - val_loss: 0.2730 - val_acc: 0.9219\n",
      "Epoch 92/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1099 - acc: 0.9863 - val_loss: 0.2705 - val_acc: 0.9231\n",
      "Epoch 93/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1093 - acc: 0.9868 - val_loss: 0.2686 - val_acc: 0.9233\n",
      "Epoch 94/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1102 - acc: 0.9865 - val_loss: 0.2694 - val_acc: 0.9233\n",
      "Epoch 95/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1105 - acc: 0.9867 - val_loss: 0.2730 - val_acc: 0.9217\n",
      "Epoch 96/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1080 - acc: 0.9864 - val_loss: 0.2713 - val_acc: 0.9227\n",
      "Epoch 97/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1080 - acc: 0.9865 - val_loss: 0.2695 - val_acc: 0.9229\n",
      "Epoch 98/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1076 - acc: 0.9867 - val_loss: 0.2705 - val_acc: 0.9225\n",
      "Epoch 99/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1110 - acc: 0.9870 - val_loss: 0.2684 - val_acc: 0.9231\n",
      "Epoch 100/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1050 - acc: 0.9871 - val_loss: 0.2707 - val_acc: 0.9242\n",
      "Epoch 101/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1058 - acc: 0.9872 - val_loss: 0.2744 - val_acc: 0.9231\n",
      "Epoch 102/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1052 - acc: 0.9868 - val_loss: 0.2700 - val_acc: 0.9221\n",
      "Epoch 103/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1047 - acc: 0.9863 - val_loss: 0.2698 - val_acc: 0.9221\n",
      "Epoch 104/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1052 - acc: 0.9874 - val_loss: 0.2688 - val_acc: 0.9237\n",
      "Epoch 105/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1048 - acc: 0.9861 - val_loss: 0.2700 - val_acc: 0.9215\n",
      "Epoch 106/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1051 - acc: 0.9867 - val_loss: 0.2726 - val_acc: 0.9221\n",
      "Epoch 107/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1045 - acc: 0.9865 - val_loss: 0.2675 - val_acc: 0.9231\n",
      "Epoch 108/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1024 - acc: 0.9873 - val_loss: 0.2688 - val_acc: 0.9221\n",
      "Epoch 109/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1021 - acc: 0.9870 - val_loss: 0.2717 - val_acc: 0.9219\n",
      "Epoch 110/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1003 - acc: 0.9881 - val_loss: 0.2696 - val_acc: 0.9223\n",
      "Epoch 111/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1015 - acc: 0.9881 - val_loss: 0.2717 - val_acc: 0.9210\n",
      "Epoch 112/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0987 - acc: 0.9877 - val_loss: 0.2722 - val_acc: 0.9206\n",
      "Epoch 113/200\n",
      "1200/1200 [==============================] - 10s 9ms/step - loss: 0.1004 - acc: 0.9878 - val_loss: 0.2721 - val_acc: 0.9206\n",
      "Epoch 114/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.1010 - acc: 0.9882 - val_loss: 0.2699 - val_acc: 0.9217\n",
      "Epoch 115/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0983 - acc: 0.9885 - val_loss: 0.2698 - val_acc: 0.9229\n",
      "Epoch 116/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0985 - acc: 0.9879 - val_loss: 0.2730 - val_acc: 0.9204\n",
      "Epoch 117/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0988 - acc: 0.9878 - val_loss: 0.2703 - val_acc: 0.9215\n",
      "Epoch 118/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0967 - acc: 0.9888 - val_loss: 0.2699 - val_acc: 0.9227\n",
      "Epoch 119/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0994 - acc: 0.9879 - val_loss: 0.2705 - val_acc: 0.9223\n",
      "Epoch 120/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0982 - acc: 0.9879 - val_loss: 0.2725 - val_acc: 0.9212\n",
      "Epoch 121/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0956 - acc: 0.9889 - val_loss: 0.2719 - val_acc: 0.9237\n",
      "Epoch 122/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0966 - acc: 0.9887 - val_loss: 0.2701 - val_acc: 0.9204\n",
      "Epoch 123/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0963 - acc: 0.9886 - val_loss: 0.2700 - val_acc: 0.9223\n",
      "Epoch 124/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0966 - acc: 0.9884 - val_loss: 0.2686 - val_acc: 0.9235\n",
      "Epoch 125/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0941 - acc: 0.9887 - val_loss: 0.2677 - val_acc: 0.9231\n",
      "Epoch 126/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0930 - acc: 0.9896 - val_loss: 0.2692 - val_acc: 0.9225\n",
      "Epoch 127/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0935 - acc: 0.9895 - val_loss: 0.2700 - val_acc: 0.9210\n",
      "Epoch 128/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0946 - acc: 0.9890 - val_loss: 0.2700 - val_acc: 0.9219\n",
      "Epoch 129/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0951 - acc: 0.9884 - val_loss: 0.2689 - val_acc: 0.9210\n",
      "Epoch 130/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0934 - acc: 0.9891 - val_loss: 0.2733 - val_acc: 0.9231\n",
      "Epoch 131/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.0970 - acc: 0.9879 - val_loss: 0.2690 - val_acc: 0.9229\n",
      "Epoch 132/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.0923 - acc: 0.9893 - val_loss: 0.2683 - val_acc: 0.9229\n",
      "Epoch 133/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.0931 - acc: 0.9894 - val_loss: 0.2689 - val_acc: 0.9242\n",
      "Epoch 134/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.0933 - acc: 0.9894 - val_loss: 0.2720 - val_acc: 0.9233\n",
      "Epoch 135/200\n",
      "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0925 - acc: 0.9901 - val_loss: 0.2700 - val_acc: 0.9227\n",
      "Epoch 136/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.0900 - acc: 0.9897 - val_loss: 0.2724 - val_acc: 0.9225\n",
      "Epoch 137/200\n",
      "1200/1200 [==============================] - 9s 8ms/step - loss: 0.0931 - acc: 0.9890 - val_loss: 0.2710 - val_acc: 0.9219\n"
     ]
    }
   ],
   "source": [
    " # starts training\n",
    "history = cnn.fit(X_train, y_train,\n",
    "                  batch_size=32, \n",
    "                  epochs=200, \n",
    "                  verbose=1,\n",
    "                  validation_data=(X_val,y_val),\n",
    "                  callbacks=callbacks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_cnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  10701300  \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (32, 1, 1, 100)           90100     \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (32, 1, 1, 100)           120100    \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (32, 1, 1, 100)           150100    \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           multiple                  1806      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,063,406\n",
      "Trainable params: 362,106\n",
      "Non-trainable params: 10,701,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.3082 - acc: 0.9112\n",
      "\n",
      "loss: 30.82%\n",
      "Accuracy: 91.12%\n"
     ]
    }
   ],
   "source": [
    "# final evaluation of the model\n",
    "print (\"\")\n",
    "scores = cnn.evaluate(X_test, y_test, verbose=1)\n",
    "print (\"\")\n",
    "print (\"loss: %.2f%%\" % (scores[0] * 100))\n",
    "print (\"Accuracy: %.2f%%\" % (scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.savefig('savefig_500dpi.png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability\n",
      "[[0.   0.   0.   0.   1.   0.  ]\n",
      " [1.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.   0.   0.   0.  ]\n",
      " ...\n",
      " [0.13 0.04 0.02 0.02 0.78 0.01]\n",
      " [0.99 0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   1.  ]]\n",
      "\n",
      "predict\n",
      "['sad' 'angry' 'fear' ... 'sad' 'angry' 'surprised']\n",
      "\n",
      "감정 개수 별 예측\n",
      "Counter({'sad': 857, 'happy': 844, 'angry': 819, 'surprised': 787, 'fear': 764, 'disgust': 729})\n"
     ]
    }
   ],
   "source": [
    "y_proba = cnn.predict(X_test)\n",
    "print(\"probability\")\n",
    "print(y_proba.round(2))\n",
    "\n",
    "\n",
    "class_names = cat_encoder.categories_[0]\n",
    "y_pred = np.argmax(y_proba,axis=1)\n",
    "print()\n",
    "print(\"predict\")\n",
    "print(class_names[y_pred])\n",
    "print()\n",
    "print(\"감정 개수 별 예측\")\n",
    "print(Counter(class_names[y_pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha\n",
    "## 함수형 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, embedding_dim = embedding_matrix.shape\n",
    "\n",
    "\n",
    "# functional\n",
    "sequence_length = X_train.shape[1]\n",
    "inputs = tf.keras.layers.Input(shape=(max_sequence_length,))\n",
    "embedding = tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM,\n",
    "                                   weights=[embedding_matrix],\n",
    "                                   trainable=True)(inputs)\n",
    "reshape = tf.keras.layers.Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "conv_0 = tf.keras.layers.Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(0.01))(reshape)\n",
    "conv_1 = tf.keras.layers.Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(0.01))(reshape)\n",
    "conv_2 = tf.keras.layers.Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = tf.keras.layers.MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), \n",
    "                                      strides=(1,1))(conv_0)\n",
    "maxpool_1 = tf.keras.layers.MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), \n",
    "                                      strides=(1,1))(conv_1)\n",
    "maxpool_2 = tf.keras.layers.MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), \n",
    "                                      strides=(1,1))(conv_2)\n",
    "\n",
    "# pool_outputs = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "pool_outputs = tf.keras.layers.Concatenate()([maxpool_0, maxpool_1, maxpool_2])\n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(pool_outputs)\n",
    "dropout = tf.keras.layers.Dropout(drop_prob)(flatten)\n",
    "output = Dense(units=6, \n",
    "               activation='softmax',\n",
    "               kernel_regularizer=tf.keras.regularizers.l2(0.01))(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = tf.keras.Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T05:04:55.795053Z",
     "start_time": "2020-07-14T05:04:55.665401Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def cnn_model(dropout):   \n",
    "    # gpu 설정\n",
    "    with K.tf.device('/gpu:0'):\n",
    "        model_Conv2 = Sequential()        \n",
    "\n",
    "        EMBEDDING_DIM = 300\n",
    "        MAX_SEQUENCE_LENGTH = 20\n",
    "        \n",
    "        # 3 Conv1D layers\n",
    "        model_Conv2.add(Embedding(len(word_index),\n",
    "                        EMBEDDING_DIM,\n",
    "                        weights = [embedding_matrix],\n",
    "                        input_length = MAX_SEQUENCE_LENGTH,\n",
    "                        trainable = False) )\n",
    "        \n",
    "        model_Conv2.add(Conv1D(256,\n",
    "                            1,\n",
    "                            padding='same',\n",
    "                            activation='relu',\n",
    "                            strides= 1))\n",
    "        model_Conv2.add(Dropout(dropout))\n",
    "        \n",
    "        model_Conv2.add(Conv1D(256,\n",
    "                            1,\n",
    "                            padding='same',\n",
    "                            activation='relu',\n",
    "                            strides= 1))\n",
    "        model_Conv2.add(Dropout(dropout))\n",
    "        \n",
    "        model_Conv2.add(Conv1D(256,\n",
    "                            1,\n",
    "                            padding='same',\n",
    "                            activation='relu',\n",
    "                            strides= 1))\n",
    "        model_Conv2.add(Dropout(dropout))\n",
    "\n",
    "        model_Conv2.add(GlobalMaxPooling1D())\n",
    "        \n",
    "        model_Conv2.add(Dense(256, activation='relu'))\n",
    "        model_Conv2.add(Dropout(dropout))\n",
    "        \n",
    "        model_Conv2.add(Dense(64, activation='relu'))\n",
    "        model_Conv2.add(Dropout(dropout))\n",
    "\n",
    "        model_Conv2.add(Dense(6, activation='softmax'))\n",
    "        \n",
    "        # optimizer 사용\n",
    "        Adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999)\n",
    "        \n",
    "        model_Conv2.compile(\n",
    "            loss = 'categorical_crossentropy',\n",
    "            optimizer = Adam,\n",
    "            metrics = ['accuracy'])\n",
    "        \n",
    "        model_Conv2.summary()\n",
    "        # Model 학습, tensorflow chart 활용\n",
    "        # tb_hist = tf.keras.callbacks.TensorBoard(log_dir = './graph', histogram_freq = 0, \n",
    "        #                                           write_graph = True, write_images = True)\n",
    "        conv_model2 = model_Conv2.fit(\n",
    "            X_train, y_train,\n",
    "            epochs = 200, batch_size = 64,\n",
    "            validation_split = 0.1,#0.1로 고정할 것\n",
    "            callbacks = [EarlyStopping(monitor = 'val_loss', patience = 10)], # tb_hist\n",
    "            verbose = 1,\n",
    "            shuffle=True)\n",
    "\n",
    "        # final evaluation of the model\n",
    "        print (\"\")\n",
    "        scores = model_Conv2.evaluate(X_test, y_test, verbose=1)\n",
    "        print (\"\")\n",
    "        print (\"loss: %.2f%%\" % (scores[0] * 100))\n",
    "        print (\"Accuracy: %.2f%%\" % (scores[1] * 100))\n",
    "\n",
    "        # 과정 살펴보기\n",
    "        matplotlib.rcParams.update({'font.size': 12})\n",
    "        fig, loss_ax = plt.subplots()\n",
    "        #krkwplt.style.use(\"ggplot\")\n",
    "        acc_ax = loss_ax.twinx()\n",
    "\n",
    "        loss_ax.plot(conv_model2.history['loss'], 'y', label= 'train loss')\n",
    "        loss_ax.plot(conv_model2.history['val_loss'], 'r', label= 'val loss')\n",
    "\n",
    "        acc_ax.plot(conv_model2.history['accuracy'], 'b', label = 'train acc')\n",
    "        acc_ax.plot(conv_model2.history['val_accuracy'], 'g', label = 'val acc')\n",
    "\n",
    "        loss_ax.set_xlabel('epoch')\n",
    "        loss_ax.set_ylabel('loss')\n",
    "        acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "        loss_ax.legend(loc='right')\n",
    "        acc_ax.legend(loc='best')\n",
    "        fig = plt.gcf()\n",
    "        plt.title(\"CNN Model\")\n",
    "        plt.show()\n",
    "        \n",
    "        return model_Conv2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
